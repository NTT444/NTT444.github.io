# Obtaining Hydrology Data with the Climata Python Package

I highly recommend you take a look at the [__Climata__](https://github.com/heigeo/climata) documentation before going too much further.  In essence, Climata allows you to easily interface with, and retrieve hydrology datasets from some prominent state and government organizations.  

To start, we will extract data for a single USGS gaging station. On the website, linked above, you can scroll down and find a table titled "Available Services"  This has important information of how you go about requesting the data from each organization.  We want USGS streamgage data so we use the class "DailyValueIO" when importing the Climata packages.

__Note:__  You could also accomplish something similar using the python package [__ulmo__](https://ulmo.readthedocs.io/en/latest/).  Additionally, you could also easily grab streamflow data with the [hydrofunctions](https://pypi.org/project/HydroFunctions/) Python package.  For this tutorial we will only focus on climata.


```python
# Extract data for one USGS gaging station
import matplotlib
import matplotlib.pyplot as plt
import climata
#### Working with climata 0.5.0 ####
from climata.usgs import DailyValueIO  # Import the relevant CLIMATA package
import pandas as pd
from pandas.plotting import register_matplotlib_converters
import numpy as np
import seaborn as sns
from IPython.display import display
from openpyxl import load_workbook
import datetime


# Designate the plot style and the width, height of the figure. 
register_matplotlib_converters()
plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = (12.0, 6.0)
```

## USGS Streamflow Data
- Next we want to select our USGS gaging station.  Navigate [here](https://waterdata.usgs.gov/nwis/rt) to find the streamgage number you want.  For this demonstration, I have selected the __Otowi gage__ on the Rio Grande in New Mexico.  

- After that, we need our "parameter id", which in my case is found [here](https://waterdata.usgs.gov/nm/nwis/dv?referred_module=sw&site_no=08313000).  The parameter id 00060 gives mean daily discharge values (in cubic feet per second).  Then we construct the appropriate list of dates we need for this timeseries.  This will extend from from the present day to the specified number of years in the past.  After that we specify the data paramter to pull out the data given out stations number, parameter ID, and date range.


```python
# Next, we want to set the requisite parameters.

nyears = 10              # You can change this parameter to adjust how many years back you want to fetch data.
ndays = 365 * nyears
station_id = "08313000"  # USGS gaging station ID
param_id = "00060"       # Mean daily discharge parameter

# Construct the appropriate list of dates we need for this timeseries 
# that extends from the present day to the specified number of years in the past.

datelist = pd.date_range(end=pd.to_datetime("today"), periods=ndays).tolist()

# Specify which dates we want to pull out of the dataset based on the datelist we outlined above
data = DailyValueIO(
    start_date=datelist[0],
    end_date=datelist[-1],
    station=station_id,
    parameter=param_id,)

print(data)
```

    <climata.usgs.DailyValueIO object at 0x000001D3706E2048>
    

The next step involves constructing a for loop, which will look through the data (established by the "data" parameter previously) and pull out all of the dates and flows within the dataset respective lists.  This for loop also takes advantage of list comprehensions (eg. flow = [r[1] for r in stream.data]), which I recommend you look up if you are like me and have a limited programming background.


```python
# Climata gives this example for displaying site information and time series data, using a traditional for loop.
#for stream in data:
    #print(stream.data)
    #for r in stream.data:
        #print(r[1])
    #for r in stream.data:
        #print(r[0])

# A more concise way of creating lists of date-flow values and putting them in a list is demonstrated below.
for stream in data:
    flow = [r[1] for r in stream.data]   #If you need, I recommend looking up list comprehensions.
    dates = [r[0] for r in stream.data]

#print(flow) 
#print(dates)  # check and see what the lists look like

# Let's combine the two lists to make a single dataframe!  That way, we can conveniently manipulate
# the data in pandas and , if we want, eventually export this to a csv, or excel file.  
# Note: All flows here are given in cubic feet per second, which is generally standard unit of 
# measure in USGS steamflow datasets.

df = pd.DataFrame({'date': dates, 
                   'Flow': flow})
display(df)
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>Flow</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2010-05-17</td>
      <td>2660.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2010-05-18</td>
      <td>3260.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2010-05-19</td>
      <td>3560.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2010-05-20</td>
      <td>3600.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2010-05-21</td>
      <td>3570.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>3644</th>
      <td>2020-05-08</td>
      <td>894.0</td>
    </tr>
    <tr>
      <th>3645</th>
      <td>2020-05-09</td>
      <td>940.0</td>
    </tr>
    <tr>
      <th>3646</th>
      <td>2020-05-10</td>
      <td>867.0</td>
    </tr>
    <tr>
      <th>3647</th>
      <td>2020-05-11</td>
      <td>886.0</td>
    </tr>
    <tr>
      <th>3648</th>
      <td>2020-05-12</td>
      <td>1090.0</td>
    </tr>
  </tbody>
</table>
<p>3649 rows × 2 columns</p>
</div>


## Cleaning and Plotting the Data

Now lets take a look at the dataframe we put together and check for any missing and/or null values.  The data cleaning you need to do will be unique to your data source and dataset; there are many situations that arise that may require other methods that to investigate and clean the data.  Fortunatley for us, the USGS streamgage data is typically in pretty good shape for most gages.  

__Note__:  The most recent data (about a month prior to the present day) undergoes a "provisional data" period before it becomes a "period of approved data".  Keep that in mind before you decide to analyze a particular timeframe of streamflow data.  In our case, we will pretent that the provisional data is "approved data" for simplicity.


```python
# I like to use one or all of the below print paramters to check out the dataset and look for any red flags
# within the dataframe.  If we notice any NaN values, empty cells, etc. we will need to clean our data before
# working with it.

print(df.dtypes)
print(df.tail())
print(df.describe())
print(df.shape)
print(df.isnull().values.any())  #check for any null values in the timeseries
print(df.isnull().sum())

# Call the plot commands to produce the hydrograph for the Otowi gage.
sns.set_style("darkgrid")
fig, ax = plt.subplots(figsize=(15,5))
plt.plot(df['date'],df['Flow'])
plt.xlabel('Date')
plt.ylabel('Streamflow in cfs')
plt.title(stream.site_name)
plt.xticks(rotation='vertical')
plt.show()

# Quick histogram plot.
print(df.hist())
```

    date    datetime64[ns]
    Flow           float64
    dtype: object
               date    Flow
    3644 2020-05-08   894.0
    3645 2020-05-09   940.0
    3646 2020-05-10   867.0
    3647 2020-05-11   886.0
    3648 2020-05-12  1090.0
                  Flow
    count  3649.000000
    mean   1123.941080
    std     892.050086
    min     232.000000
    25%     668.000000
    50%     868.000000
    75%    1170.000000
    max    6540.000000
    (3649, 2)
    False
    date    0
    Flow    0
    dtype: int64
    


![png](output_7_1.png)


    [[<matplotlib.axes._subplots.AxesSubplot object at 0x000001D3706F2548>]]
    


![png](output_7_3.png)


## Distribution of Streamflow Values

First, let's the distrubution of streamflow at the Otowi Gage in New Mexico.  We can do this with matplotlib or seaborn; in this case I opt for seaborn.


```python
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

sns.set(style="ticks")
x=df['Flow']

f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, 
                                    gridspec_kw={"height_ratios": (.15, .85)}, figsize=(15, 8))

sns.boxplot(x, ax=ax_box, showmeans=True)
sns.distplot(x, ax=ax_hist)

# Remove the border around the figures
ax_box.set(yticks=[])
sns.despine(ax=ax_hist)
sns.despine(ax=ax_box, left=True)
```


![png](output_9_0.png)


## Plot Flow Duration Curve

Flow duration curves are defined using the following equation:

__P = 100 * [ M / (n + 1) ]__

where:
- __P__ = the probability that a given flow will be equaled or exceeded (% of time)
- __M__ = the ranked position on the listing (dimensionless)
- __n__ = the number of events for period of record (dimensionless)


```python
df['pct_rank'] = df['Flow'].rank(ascending=False,pct=True)
df['pct_rank'] = df['pct_rank']
df = df.sort_values(by=['pct_rank'])
print(df.head())

sns.set_style("darkgrid")
fig, ax = plt.subplots(figsize=(15,8))
plt.plot(df['pct_rank'], df['Flow'], linewidth=2.0)
plt.xlabel('Exceedence Probability (Percent of time that indicated dischage was equaled or exceeded)')
plt.ylabel('Streamflow in cfs')
plt.title('Flow Duration Curve at the Otowi Gage on the Rio Grande')
```

               date    Flow  pct_rank
    3319 2019-06-18  6540.0  0.000274
    3318 2019-06-17  6390.0  0.000548
    3317 2019-06-16  6280.0  0.000822
    3315 2019-06-14  6220.0  0.001096
    3320 2019-06-19  6170.0  0.001370
    




    Text(0.5, 1.0, 'Flow Duration Curve at the Otowi Gage on the Rio Grande')




![png](output_11_2.png)


## Query with FIPS Code

We can also query the data using Federal Information Processing Standard (FIPS) county codes.  You can find a complete list of FIPS county codes [here](https://www.nrcs.usda.gov/wps/portal/nrcs/detail/national/home/?cid=nrcs143_013697) and pick the one you desire.  For this demonstration I selected the Santa Fe County FIPS code. 

If you run the code below, and exclude the site name filter, you will get a long list of items.  For this example, let's say we do not want the piezometers or the items that have numbers for the site names (meaning we only want USGS stream gage data).  To exclude the piezometers, we can maintain site names that only have upper case letters.  Then, we can exclude the site names that have numbers for their label.  This will leave us with site names that are consistent with streamgages from the USGS.


```python
#%%  Extract Data using FIPS code
# Set the parameters
nyears = 1
ndays = 365 * nyears
county = "35049"  # Enter FIPS code for county
datelist = pd.date_range(end=pd.to_datetime("today"), periods=ndays).tolist()

data = DailyValueIO(
    start_date=datelist[0],
    end_date=datelist[-1],
    county=county)

date = []
value = []

# Get the date and values into list using traditional for loop
for series in data:
    for row in series.data:
        date.append(row[0])
        value.append(row[1])
        
site_names = [[series.site_name] * len(series.data) for series in data]
station_id = [[series.site_code] * len(series.data) for series in data]

# Unroll the list of lists using a list comprehension.
flat_site_names = [item for sublist in site_names for item in sublist]
flat_station_id = [item2 for sublist2 in station_id for item2 in sublist2]

# Bundle the data into a data frame
df = pd.DataFrame({'site': flat_site_names,
                   'station_id': flat_station_id, 
                   'date': date, 
                   'value': value})

# Filtering by site names - Toggle these on and off on your own to see what they do!
# Keep site names that have all upper case letters
df = df[df['site'].str[0:].str.isupper()]
#df = df[df['site'].str[0:].str.isalpha()]
# Get rid of site names that contain the word Reservoir, Note:  ~ serves as a "not" boolean
df = df[~df["site"].str.contains('RESERVOIR', na=False)]
# Filter out site names that start with 19N
df = df[~df["site"].str.contains('19N', na=False)]

display(df.tail())
print(df.shape)
print(df.isnull().values.any())  #check for any null values in the timeseries
print(df.isnull().sum())
display(df)

#df = df[df['value'] != -999999.0]
print(df.shape)
# Remove missing values and use backfill method
df = df.replace(-999999.0, np.nan)
df = df.dropna()
#df = df.fillna(method = "bfill")
display(df.tail())

df = df.set_index(['date'])
#print(df.tail())

# Visualize flow time series and color by site name
grouptest=df.groupby(['site','station_id'])
#print(grouptest)

groups = df.groupby('site')

fig, ax = plt.subplots()
for name, group in groups:
    ax.plot(group.index, group.value, marker='o', linestyle='-', ms=2, label=name)
ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.xlabel('Date')
plt.ylabel('Streamflow in cfs')
plt.xticks(rotation='vertical')
plt.show()
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>site</th>
      <th>station_id</th>
      <th>date</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>5672</th>
      <td>GALISTEO CREEK BELOW GALISTEO DAM, NM</td>
      <td>08317950</td>
      <td>2020-05-08</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5673</th>
      <td>GALISTEO CREEK BELOW GALISTEO DAM, NM</td>
      <td>08317950</td>
      <td>2020-05-09</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5674</th>
      <td>GALISTEO CREEK BELOW GALISTEO DAM, NM</td>
      <td>08317950</td>
      <td>2020-05-10</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5675</th>
      <td>GALISTEO CREEK BELOW GALISTEO DAM, NM</td>
      <td>08317950</td>
      <td>2020-05-11</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5676</th>
      <td>GALISTEO CREEK BELOW GALISTEO DAM, NM</td>
      <td>08317950</td>
      <td>2020-05-12</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>


    (3863, 4)
    False
    site          0
    station_id    0
    date          0
    value         0
    dtype: int64
    


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>site</th>
      <th>station_id</th>
      <th>date</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>SANTA CRUZ RIVER NEAR CUNDIYO, NM</td>
      <td>08291000</td>
      <td>2019-05-15</td>
      <td>77.1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>SANTA CRUZ RIVER NEAR CUNDIYO, NM</td>
      <td>08291000</td>
      <td>2019-05-16</td>
      <td>94.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>SANTA CRUZ RIVER NEAR CUNDIYO, NM</td>
      <td>08291000</td>
      <td>2019-05-17</td>
      <td>116.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>SANTA CRUZ RIVER NEAR CUNDIYO, NM</td>
      <td>08291000</td>
      <td>2019-05-18</td>
      <td>124.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>SANTA CRUZ RIVER NEAR CUNDIYO, NM</td>
      <td>08291000</td>
      <td>2019-05-19</td>
      <td>112.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>5672</th>
      <td>GALISTEO CREEK BELOW GALISTEO DAM, NM</td>
      <td>08317950</td>
      <td>2020-05-08</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5673</th>
      <td>GALISTEO CREEK BELOW GALISTEO DAM, NM</td>
      <td>08317950</td>
      <td>2020-05-09</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5674</th>
      <td>GALISTEO CREEK BELOW GALISTEO DAM, NM</td>
      <td>08317950</td>
      <td>2020-05-10</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5675</th>
      <td>GALISTEO CREEK BELOW GALISTEO DAM, NM</td>
      <td>08317950</td>
      <td>2020-05-11</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5676</th>
      <td>GALISTEO CREEK BELOW GALISTEO DAM, NM</td>
      <td>08317950</td>
      <td>2020-05-12</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>3863 rows × 4 columns</p>
</div>


    (3863, 4)
    


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>site</th>
      <th>station_id</th>
      <th>date</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>5672</th>
      <td>GALISTEO CREEK BELOW GALISTEO DAM, NM</td>
      <td>08317950</td>
      <td>2020-05-08</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5673</th>
      <td>GALISTEO CREEK BELOW GALISTEO DAM, NM</td>
      <td>08317950</td>
      <td>2020-05-09</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5674</th>
      <td>GALISTEO CREEK BELOW GALISTEO DAM, NM</td>
      <td>08317950</td>
      <td>2020-05-10</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5675</th>
      <td>GALISTEO CREEK BELOW GALISTEO DAM, NM</td>
      <td>08317950</td>
      <td>2020-05-11</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5676</th>
      <td>GALISTEO CREEK BELOW GALISTEO DAM, NM</td>
      <td>08317950</td>
      <td>2020-05-12</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>



![png](output_13_5.png)



```python
#Let's Reshape the data first (sites as columns, dates as index)
df = df.reset_index()
dfg = df.groupby(['date','site'], as_index=False).sum()
dfp = dfg.pivot(index='date', columns='site',values='value')
display(dfp)
#dfp.to_excel(r'C:\Users\......') This will save it to an excel spreadsheet, if you want...
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>site</th>
      <th>GALISTEO CREEK BELOW GALISTEO DAM, NM</th>
      <th>RIO GRANDE AT OTOWI BRIDGE, NM</th>
      <th>RIO NAMBE ABOVE NAMBE FALLS DAM NEAR NAMBE, NM</th>
      <th>RIO NAMBE BELOW NAMBE FALLS DAM NEAR NAMBE, NM</th>
      <th>RIO TESUQUE BELOW DIVERSIONS NEAR SANTA FE, NM</th>
      <th>SANTA CRUZ RIVER NEAR CUNDIYO, NM</th>
      <th>SANTA FE RIVER ABOVE COCHITI LAKE, NM</th>
      <th>SANTA FE RIVER ABOVE MCCLURE RES, NR SANTA FE, NM</th>
      <th>SANTA FE RIVER NEAR SANTA FE, NM</th>
      <th>TESUQUE CREEK ABOVE DIVERSIONS NEAR SANTA FE, NM</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2019-05-15</th>
      <td>0.0</td>
      <td>4670.0</td>
      <td>31.2</td>
      <td>26.7</td>
      <td>6.19</td>
      <td>77.1</td>
      <td>10.70</td>
      <td>20.3</td>
      <td>8.55</td>
      <td>7.42</td>
    </tr>
    <tr>
      <th>2019-05-16</th>
      <td>0.0</td>
      <td>4750.0</td>
      <td>34.4</td>
      <td>29.0</td>
      <td>6.07</td>
      <td>94.6</td>
      <td>9.62</td>
      <td>21.5</td>
      <td>7.38</td>
      <td>7.72</td>
    </tr>
    <tr>
      <th>2019-05-17</th>
      <td>0.0</td>
      <td>5070.0</td>
      <td>36.2</td>
      <td>30.3</td>
      <td>8.02</td>
      <td>116.0</td>
      <td>9.20</td>
      <td>25.3</td>
      <td>5.99</td>
      <td>8.49</td>
    </tr>
    <tr>
      <th>2019-05-18</th>
      <td>0.0</td>
      <td>5310.0</td>
      <td>36.4</td>
      <td>30.1</td>
      <td>9.57</td>
      <td>124.0</td>
      <td>8.02</td>
      <td>27.2</td>
      <td>6.11</td>
      <td>9.17</td>
    </tr>
    <tr>
      <th>2019-05-19</th>
      <td>0.0</td>
      <td>5490.0</td>
      <td>35.9</td>
      <td>31.1</td>
      <td>9.66</td>
      <td>112.0</td>
      <td>7.02</td>
      <td>26.8</td>
      <td>6.11</td>
      <td>9.20</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2020-05-09</th>
      <td>0.0</td>
      <td>940.0</td>
      <td>22.0</td>
      <td>25.8</td>
      <td>2.83</td>
      <td>70.8</td>
      <td>4.70</td>
      <td>27.8</td>
      <td>10.50</td>
      <td>2.89</td>
    </tr>
    <tr>
      <th>2020-05-10</th>
      <td>0.0</td>
      <td>867.0</td>
      <td>22.1</td>
      <td>25.4</td>
      <td>2.55</td>
      <td>65.4</td>
      <td>2.42</td>
      <td>25.5</td>
      <td>10.50</td>
      <td>2.84</td>
    </tr>
    <tr>
      <th>2020-05-11</th>
      <td>0.0</td>
      <td>886.0</td>
      <td>22.5</td>
      <td>27.1</td>
      <td>2.80</td>
      <td>65.0</td>
      <td>2.33</td>
      <td>24.5</td>
      <td>10.70</td>
      <td>3.01</td>
    </tr>
    <tr>
      <th>2020-05-12</th>
      <td>0.0</td>
      <td>1090.0</td>
      <td>22.3</td>
      <td>26.0</td>
      <td>2.68</td>
      <td>61.3</td>
      <td>2.54</td>
      <td>21.9</td>
      <td>10.70</td>
      <td>2.86</td>
    </tr>
    <tr>
      <th>2020-05-13</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.80</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>365 rows × 10 columns</p>
</div>


## Query by Basin (aka. Watershed/HUC boundary)

Perhaps the most useful application, for water resource oriented folks, is obtaining streamflow data by watershed.  Let's take a brief look at how to get streamflow values using a Hydrologic Unit Code (HUC).

First, I suggest you look at how the [HUCs](https://water.usgs.gov/GIS/huc_name.html#) are broken down.  Essentially the HUC defines [your potential watershed](https://water.usgs.gov/wsc/map_index.html) at different scales (or resolutions).  This will help you locate the appropriate watershed boundary.

Again, we apply a filter to remove piezometers and smaller tributary flows.  This generally reveals the flows that are of higher magnitude.


```python
# set parameters
nyears = 3
ndays = 365 * nyears
basin = "13020101"
param_id = "00060" 
datelist = pd.date_range(end=pd.to_datetime("today"), periods=ndays,freq='D').tolist()
#datelist = pd.date_range(end=2019-31-12, periods=ndays,freq='D').tolist()
#datelist = pd.date_range(start='2018-01-01', end='2019-12-31', freq='D')
data = DailyValueIO(
    start_date=datelist[0],
    end_date=datelist[-1],
    basin=basin,
    parameter=param_id,)

date = []
value = []

for series in data:
    for row in series.data:
        date.append(row[0])
        value.append(row[1])
        
print(series)
site_names = [[series.site_name] * len(series.data) for series in data]

# Unroll the list of lists
flat_site_names = [item for sublist in site_names for item in sublist]
# Bundle the data into a data frame
df = pd.DataFrame({'site': flat_site_names, 
                   'date': date, 
                   'value': value})
display(df.head())
df = df[df['site'].str[0:].str.isupper()]
# Get rid of site names that contain the word Reservoir, Note:  ~ serves as a "not" boolean
#df = df[df["site"].str.contains('RIO GRANDE')]
#df = df[~df["site"].str.contains('RESERVOIR', na=False)]
df = df[~df["site"].str.contains('CREEK', na=False)]

df = df.replace(-999999.0, np.nan)
df = df.dropna()

df1 = df.reset_index()
dfg = df1.groupby(['date','site'], as_index=False).sum()
dfp = dfg.pivot(index='date', columns='site',values='value')
display(dfp.head())
#dfp.to_excel(r'C:\Users\......')  Send to Excel if you prefer.

groups = df.groupby('site')

fig, ax = plt.subplots()
for name, group in groups:
    ax.plot(group.date, group.value, marker='o', linestyle='-', ms=2, label=name)
ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.xlabel('Date')
plt.ylabel('Streamflow')
plt.xticks(rotation='vertical')
plt.tight_layout
plt.show()
```

    DailyValueIOTuple(site_name='Rio Grande abv Buckman Diversion, nr White Rock,NM', site_code='08313150', variable_name='Streamflow, ft&#179;/s', variable_code='00060', unit='ft3/s', latitude=35.83841667, longitude=-106.1590833, data=<climata.parsers.TimeSeriesIO object at 0x000001D3746C0E08>)
    


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>site</th>
      <th>date</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>COSTILLA CREEK ABOVE COSTILLA DAM, NM</td>
      <td>2017-05-15</td>
      <td>36.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>COSTILLA CREEK ABOVE COSTILLA DAM, NM</td>
      <td>2017-05-16</td>
      <td>35.9</td>
    </tr>
    <tr>
      <th>2</th>
      <td>COSTILLA CREEK ABOVE COSTILLA DAM, NM</td>
      <td>2017-05-17</td>
      <td>34.5</td>
    </tr>
    <tr>
      <th>3</th>
      <td>COSTILLA CREEK ABOVE COSTILLA DAM, NM</td>
      <td>2017-05-18</td>
      <td>30.3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>COSTILLA CREEK ABOVE COSTILLA DAM, NM</td>
      <td>2017-05-19</td>
      <td>28.0</td>
    </tr>
  </tbody>
</table>
</div>



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>site</th>
      <th>RED RIVER BELOW FISH HATCHERY, NEAR QUESTA, NM</th>
      <th>RED RIVER NEAR QUESTA, NM</th>
      <th>RIO GRANDE AT EMBUDO, NM</th>
      <th>RIO GRANDE AT OTOWI BRIDGE, NM</th>
      <th>RIO GRANDE BLW TAOS JUNCTION BRIDGE NEAR TAOS, NM</th>
      <th>RIO GRANDE DEL RANCHO NEAR TALPA, NM</th>
      <th>RIO GRANDE NEAR CERRO, NM</th>
      <th>RIO HONDO NEAR VALDEZ, NM</th>
      <th>RIO LUCERO NEAR ARROYO SECO, NM</th>
      <th>RIO NAMBE ABOVE NAMBE FALLS DAM NEAR NAMBE, NM</th>
      <th>RIO NAMBE BELOW NAMBE FALLS DAM NEAR NAMBE, NM</th>
      <th>RIO PUEBLO DE TAOS BELOW LOS CORDOVAS, NM</th>
      <th>RIO PUEBLO DE TAOS NEAR TAOS, NM</th>
      <th>RIO PUEBLO NR PENASCO, NM</th>
      <th>RIO TESUQUE BELOW DIVERSIONS NEAR SANTA FE, NM</th>
      <th>SANTA CRUZ RIVER NEAR CUNDIYO, NM</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2017-05-15</th>
      <td>336.0</td>
      <td>265.0</td>
      <td>3080.0</td>
      <td>4690.0</td>
      <td>2700.0</td>
      <td>270.0</td>
      <td>1500.0</td>
      <td>152.0</td>
      <td>68.4</td>
      <td>36.0</td>
      <td>40.5</td>
      <td>650.0</td>
      <td>193.0</td>
      <td>401.0</td>
      <td>7.22</td>
      <td>95.5</td>
    </tr>
    <tr>
      <th>2017-05-16</th>
      <td>346.0</td>
      <td>264.0</td>
      <td>3390.0</td>
      <td>5380.0</td>
      <td>3080.0</td>
      <td>254.0</td>
      <td>1830.0</td>
      <td>153.0</td>
      <td>70.4</td>
      <td>36.5</td>
      <td>42.6</td>
      <td>636.0</td>
      <td>181.0</td>
      <td>348.0</td>
      <td>6.83</td>
      <td>89.1</td>
    </tr>
    <tr>
      <th>2017-05-17</th>
      <td>331.0</td>
      <td>253.0</td>
      <td>3610.0</td>
      <td>5700.0</td>
      <td>3300.0</td>
      <td>220.0</td>
      <td>2040.0</td>
      <td>147.0</td>
      <td>71.9</td>
      <td>37.2</td>
      <td>42.5</td>
      <td>620.0</td>
      <td>162.0</td>
      <td>282.0</td>
      <td>6.80</td>
      <td>87.5</td>
    </tr>
    <tr>
      <th>2017-05-18</th>
      <td>311.0</td>
      <td>229.0</td>
      <td>3650.0</td>
      <td>5620.0</td>
      <td>3390.0</td>
      <td>187.0</td>
      <td>2250.0</td>
      <td>137.0</td>
      <td>68.7</td>
      <td>38.6</td>
      <td>41.9</td>
      <td>547.0</td>
      <td>140.0</td>
      <td>238.0</td>
      <td>7.17</td>
      <td>88.4</td>
    </tr>
    <tr>
      <th>2017-05-19</th>
      <td>290.0</td>
      <td>201.0</td>
      <td>3640.0</td>
      <td>5640.0</td>
      <td>3400.0</td>
      <td>156.0</td>
      <td>2190.0</td>
      <td>127.0</td>
      <td>61.8</td>
      <td>38.0</td>
      <td>39.6</td>
      <td>476.0</td>
      <td>121.0</td>
      <td>210.0</td>
      <td>8.30</td>
      <td>82.9</td>
    </tr>
  </tbody>
</table>
</div>



![png](output_16_3.png)


## Chart that shows where there is valid streamflow data
### (Similar to a Gantt Chart)

This chart allows us to visualize the gaps in our data and where we had valid data.  This could be due to gage malfunction, maintenace, removal, etc.  

You can also check out the [__missingno__](https://www.geeksforgeeks.org/python-visualize-missing-values-nan-values-using-missingno-library/) python library that has a similar sort of functionality built in.  Basically it allows you to quickly visualize where you have data gaps.  I find this helpful as I tend to learn and think visually.


```python
dfp.sort_index(inplace=True)

from datetime import datetime
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()

# Loop through and find the good data rangees (i.e. the ones we want to plot)
good_ranges = []
for i in dfp:
    col = dfp[i]
    gauge_name = col.name

    # Start of good data block defined by a number preceeded by a NaN
    start_mark = (col.notnull() & col.shift().isnull())
    start = col[start_mark].index

    # End of good data block defined by a number followed by a Nan
    end_mark = (col.notnull() & col.shift(-1).isnull())
    end = col[end_mark].index

    for s, e in zip(start, end):
        good_ranges.append((gauge_name, s, e))

good_ranges = pd.DataFrame(good_ranges, columns=['gauge', 'start', 'end'])
#print(good_ranges)

idx = (len(good_ranges))
idx_un = len(pd.unique(good_ranges['gauge']))

# Only label every 30th value
ticks_to_use = dfp.index[::30]

# Set format of labels (note year not excluded as requested)
labels = [i.strftime("%m/%d/%Y") for i in ticks_to_use]
colors = plt.cm.bone(np.linspace(0,1,idx))
# Plotting
fig, ax = plt.subplots(figsize=(14,10))
ax.grid(which='both', axis='x', linestyle='--',color='gray')
ax.set_axisbelow(True)

ax.hlines(good_ranges['gauge'],(good_ranges['start']), good_ranges['end'],linewidth=2.5)#, color=colors)
ax.set_xticks(ticks_to_use)
ax.set_xticklabels(labels)
plt.xticks(rotation='vertical')

fig.tight_layout()
plt.show()
```


![png](output_18_0.png)



```python
# Another Method to do the same thing as above.

new_rows = [dfp[s].where(dfp[s].isna(), i) for i, s in enumerate(dfp, 1)]
# To increase spacing between lines add a number to i, eg. below:
# [df[s].where(df[s].isna(), i+3) for i, s in enumerate(df, 1)]
new_df = pd.DataFrame(new_rows)
#print(new_df.head())
### Plotting ###
n=len(new_df)
colors = plt.cm.nipy_spectral(np.linspace(0,1,n))

sns.set_style('white')
fig, ax = plt.subplots() # Create axes object to pass to pandas df.plot()
ax = new_df.transpose().plot(figsize=(10,10), ax=ax, legend=False, fontsize=12, linewidth=2, color=colors)
list_of_sites = new_df.transpose().columns.to_list() # For y tick labels
x_tick_location = new_df.iloc[:, 0].values # For y tick positions
ax.set_yticks(x_tick_location) # Place ticks in correct positions
ax.set_yticklabels(list_of_sites) # Update labels to site names
#plt.tight_layout
plt.show()
```


![png](output_19_0.png)


## ACIS Data by Basin Number

Nice, now let's grab some temperature data (deg F).  This time we will pick the temperature data from [ACIS](http://www.rcc-acis.org/docs_webservices.html).
We will also need to find the basin ID number.  In this case it is the 8-digit nubmer found for [Subregion 13](https://water.usgs.gov/GIS/huc_name.html#Region13).




```python
import matplotlib
import matplotlib.pyplot as plt
import pandas as pd
from pandas.plotting import register_matplotlib_converters
import numpy as np
from climata.acis import StationDataIO

register_matplotlib_converters()
plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = (20.0, 10.0)
# set parameters
nyears = 2
month = 365 * nyears
datelist = pd.date_range(end=pd.to_datetime("today"), periods=month).tolist()
# Load average temperature for sites in Rio Grande Subregion 1302
#Include Rio Grande around the Santa Fe Region.
sites = StationDataIO(
    basin = "13020201",
    start_date = "2017-01-01",
    end_date = "2017-12-31",
    parameter = "avgt")

date = []
value = []
# Display site information and time series data, using a typical for loop
for site in sites:
    #print(site.name)
    for evt in site.data:
        #print(evt.date, evt.avgt)
        date.append(evt[0])
        value.append(evt[1])
        
# Use a list comprehension to get the site names
site_names = [[site.name] * len(site.data) for site in sites]

# Unroll the list of lists
flat_site_names = [item for sublist in site_names for item in sublist]

#print(flat_site_names)

df = pd.DataFrame({'site': flat_site_names, 
                   'date': date, 
                   'value': value})
#print(df)
# remove missing values
df = df[df['value'] != -999999.0]
print(df.shape)
df = df.replace('M', np.nan)

df = df.fillna(method = "bfill")
display(df)

##df = df[df['value'] != 'M']
##print(df.shape)
# visualize flow time series, coloring by site
groups = df.groupby('site')
#groups.set_index('date', inplace=True)
#print(groups.head())
#groups_m = df.resample('M').mean()
#print(groups_m)
fig, ax = plt.subplots(figsize=(15,8))
for name, group in groups:
    ax.plot(group.date, group.value, marker='o', linestyle='-', ms=2, label=name)
ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.xlabel('Date')
plt.ylabel('Average Temp')
plt.xticks(rotation='vertical')
plt.show()
```

    (3285, 3)
    


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>site</th>
      <th>date</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>COCHITI DAM</td>
      <td>2017-01-01</td>
      <td>37.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>COCHITI DAM</td>
      <td>2017-01-02</td>
      <td>37.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>COCHITI DAM</td>
      <td>2017-01-03</td>
      <td>37.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>COCHITI DAM</td>
      <td>2017-01-04</td>
      <td>39.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>COCHITI DAM</td>
      <td>2017-01-05</td>
      <td>41.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>3280</th>
      <td>SANTA FE</td>
      <td>2017-12-27</td>
      <td>31.0</td>
    </tr>
    <tr>
      <th>3281</th>
      <td>SANTA FE</td>
      <td>2017-12-28</td>
      <td>32.5</td>
    </tr>
    <tr>
      <th>3282</th>
      <td>SANTA FE</td>
      <td>2017-12-29</td>
      <td>33.5</td>
    </tr>
    <tr>
      <th>3283</th>
      <td>SANTA FE</td>
      <td>2017-12-30</td>
      <td>38.5</td>
    </tr>
    <tr>
      <th>3284</th>
      <td>SANTA FE</td>
      <td>2017-12-31</td>
      <td>32.5</td>
    </tr>
  </tbody>
</table>
<p>3285 rows × 3 columns</p>
</div>



![png](output_21_2.png)


Interesting, there seems to be a few issues with a couple of the temperature data locations.  First, let's pivot our data to go from a "tall" DataFrame to a "wide" DataFrame.  Let's take a look:


```python
df_wide = df.pivot(index='date', columns='site', values='value')
display(df_wide)

df_wide[['PLACITAS','COCHITI DAM']].plot()
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>site</th>
      <th>COCHITI DAM</th>
      <th>ELK CABIN</th>
      <th>LOS ALAMOS</th>
      <th>PLACITAS</th>
      <th>SANTA FE</th>
      <th>SANTA FE 2</th>
      <th>SANTA FE 20 WNW</th>
      <th>SANTA FE COUNTY MUNICIPAL AP</th>
      <th>SANTA FE SETON</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2017-01-01</th>
      <td>37.0</td>
      <td>31.5</td>
      <td>34.5</td>
      <td>62.0</td>
      <td>21.5</td>
      <td>36.5</td>
      <td>33.0</td>
      <td>36.0</td>
      <td>34.0</td>
    </tr>
    <tr>
      <th>2017-01-02</th>
      <td>37.0</td>
      <td>30.5</td>
      <td>32.5</td>
      <td>62.0</td>
      <td>19.5</td>
      <td>31.0</td>
      <td>35.0</td>
      <td>33.0</td>
      <td>35.0</td>
    </tr>
    <tr>
      <th>2017-01-03</th>
      <td>37.0</td>
      <td>33.5</td>
      <td>34.5</td>
      <td>62.0</td>
      <td>20.5</td>
      <td>33.5</td>
      <td>32.5</td>
      <td>36.0</td>
      <td>33.0</td>
    </tr>
    <tr>
      <th>2017-01-04</th>
      <td>39.0</td>
      <td>31.5</td>
      <td>35.0</td>
      <td>62.0</td>
      <td>21.5</td>
      <td>34.5</td>
      <td>34.5</td>
      <td>36.0</td>
      <td>35.5</td>
    </tr>
    <tr>
      <th>2017-01-05</th>
      <td>41.0</td>
      <td>27.5</td>
      <td>28.5</td>
      <td>62.0</td>
      <td>22.5</td>
      <td>35.5</td>
      <td>36.5</td>
      <td>29.5</td>
      <td>38.5</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2017-12-27</th>
      <td>41.0</td>
      <td>39.5</td>
      <td>39.0</td>
      <td>33.0</td>
      <td>31.0</td>
      <td>41.5</td>
      <td>34.5</td>
      <td>41.5</td>
      <td>41.0</td>
    </tr>
    <tr>
      <th>2017-12-28</th>
      <td>41.0</td>
      <td>40.5</td>
      <td>40.0</td>
      <td>33.0</td>
      <td>32.5</td>
      <td>36.0</td>
      <td>34.5</td>
      <td>37.5</td>
      <td>40.5</td>
    </tr>
    <tr>
      <th>2017-12-29</th>
      <td>62.0</td>
      <td>40.5</td>
      <td>41.5</td>
      <td>33.0</td>
      <td>33.5</td>
      <td>35.0</td>
      <td>34.5</td>
      <td>37.0</td>
      <td>39.5</td>
    </tr>
    <tr>
      <th>2017-12-30</th>
      <td>62.0</td>
      <td>41.5</td>
      <td>43.0</td>
      <td>33.0</td>
      <td>38.5</td>
      <td>39.0</td>
      <td>34.5</td>
      <td>41.5</td>
      <td>40.5</td>
    </tr>
    <tr>
      <th>2017-12-31</th>
      <td>62.0</td>
      <td>35.5</td>
      <td>33.5</td>
      <td>33.0</td>
      <td>32.5</td>
      <td>36.5</td>
      <td>34.5</td>
      <td>32.0</td>
      <td>43.5</td>
    </tr>
  </tbody>
</table>
<p>365 rows × 9 columns</p>
</div>





    <matplotlib.axes._subplots.AxesSubplot at 0x1d370381508>




![png](output_23_2.png)


Since we used backfill (.bfill method) we are getting the first non-nan value that is back projected through the timeseries, hence the flat red line at 62 degrees F.  It looks like the station at Placitas is not a reliable gage so we may want to exclude this from our analysis.  Likewise, the last 3 days of the Cochiti Dam temperature look anamolously high for January.  We could replace the value here with the seasonal average, if that is indeed a gage issue.  What you do would largely depend on what you are looking to analyze with this data.


```python
# Bonus: Scratch Work from when I was playing around with the data.
# set parameters
nyears = 3
ndays = 365 * nyears
basin = "13020101"
param_id = "00060" 
datelist = pd.date_range(end=pd.to_datetime("today"), periods=ndays,freq='D').tolist()
data = DailyValueIO(
    start_date=datelist[0],
    end_date=datelist[-1],
    basin=basin,
    parameter=param_id,)

date = []
value = []

for series in data:
    for row in series.data:
        date.append(row[0])
        value.append(row[1])
        
print(series)
site_names = [[series.site_name] * len(series.data) for series in data]

# unroll the list of lists
flat_site_names = [item for sublist in site_names for item in sublist]
# bundle the data into a data frame
df = pd.DataFrame({'site': flat_site_names, 
                   'date': date, 
                   'value': value})
print(df.head())
df = df[df['site'].str[0:].str.isupper()]
# Get rid of site names that contain the word Reservoir, Note:  ~ serves as a "not" boolean
df = df[df["site"].str.contains('RIO GRANDE')]
#df = df[~df["site"].str.contains('RESERVOIR', na=False)]
#df = df[~df["site"].str.contains('CREEK', na=False)]
# Filter out site names that start with 19N
#df = df[~df["site"].str.contains('30N', na=False)]

# remove missing values
#df = df[df['value'] != -999999.0]
df = df.replace(-999999.0, np.nan)
df = df.dropna()
#df = df.fillna(method = "bfill")

#
df1 = df.reset_index()
dfg = df1.groupby(['date','site'], as_index=False).sum()
dfp = dfg.pivot(index='date', columns='site',values='value')
display(dfp.head())
print(dfp.plot())

groups = df.groupby('site')

fig, ax = plt.subplots()
for name, group in groups:
    ax.plot(group.date, group.value, marker='o', linestyle='-', ms=2, label=name)
ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.xlabel('Date')
plt.ylabel('Streamflow')
#plt.xticks(rotation='vertical')
plt.tight_layout
plt.show()
```

    DailyValueIOTuple(site_name='Rio Grande abv Buckman Diversion, nr White Rock,NM', site_code='08313150', variable_name='Streamflow, ft&#179;/s', variable_code='00060', unit='ft3/s', latitude=35.83841667, longitude=-106.1590833, data=<climata.parsers.TimeSeriesIO object at 0x000001D3756F3608>)
                                        site       date  value
    0  COSTILLA CREEK ABOVE COSTILLA DAM, NM 2017-05-15   36.2
    1  COSTILLA CREEK ABOVE COSTILLA DAM, NM 2017-05-16   35.9
    2  COSTILLA CREEK ABOVE COSTILLA DAM, NM 2017-05-17   34.5
    3  COSTILLA CREEK ABOVE COSTILLA DAM, NM 2017-05-18   30.3
    4  COSTILLA CREEK ABOVE COSTILLA DAM, NM 2017-05-19   28.0
    


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>site</th>
      <th>RIO GRANDE AT EMBUDO, NM</th>
      <th>RIO GRANDE AT OTOWI BRIDGE, NM</th>
      <th>RIO GRANDE BLW TAOS JUNCTION BRIDGE NEAR TAOS, NM</th>
      <th>RIO GRANDE DEL RANCHO NEAR TALPA, NM</th>
      <th>RIO GRANDE NEAR CERRO, NM</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2017-05-15</th>
      <td>3080.0</td>
      <td>4690.0</td>
      <td>2700.0</td>
      <td>270.0</td>
      <td>1500.0</td>
    </tr>
    <tr>
      <th>2017-05-16</th>
      <td>3390.0</td>
      <td>5380.0</td>
      <td>3080.0</td>
      <td>254.0</td>
      <td>1830.0</td>
    </tr>
    <tr>
      <th>2017-05-17</th>
      <td>3610.0</td>
      <td>5700.0</td>
      <td>3300.0</td>
      <td>220.0</td>
      <td>2040.0</td>
    </tr>
    <tr>
      <th>2017-05-18</th>
      <td>3650.0</td>
      <td>5620.0</td>
      <td>3390.0</td>
      <td>187.0</td>
      <td>2250.0</td>
    </tr>
    <tr>
      <th>2017-05-19</th>
      <td>3640.0</td>
      <td>5640.0</td>
      <td>3400.0</td>
      <td>156.0</td>
      <td>2190.0</td>
    </tr>
  </tbody>
</table>
</div>


    AxesSubplot(0.125,0.125;0.775x0.755)
    


![png](output_25_3.png)



![png](output_25_4.png)


# Credits & Acknowledgements

As usual, I do not just automatically know how to instantly code everything; I expanded upon examples that already existed to arrive at the code in this notebook.  Much of the code in this notebook was built upon an example found at [__earthdatascience.org__](https://www.earthdatascience.org/tutorials/acquire-and-visualize-usgs-hydrology-data/).

Other resources I used for my code were:
* Python documentation for the respective packages
* The [__stackoverflow__](https://stackoverflow.com/) community.
* And last, but not least, the folks who put together the [__Climata__](https://github.com/heigeo/climata) package.  
